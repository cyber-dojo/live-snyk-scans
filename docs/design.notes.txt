
It would be useful if each repo could still have its own snyk scan if desired.
Could be a separate workflow? One issue is that if the main workflow runs 
then it will create a docker image in the registry with an immutable tag,
so the snyk workflow would not be able to build its image in the same way.

Could the main workflow have a snyk-container-scan job that does not participate 
in the sdlc-control-gate and so does not affect deployment?
This would mean the scan is definitely on the correct docker image.

I could have Trails in a Flow=aws-snyk-scan whose names are taken from
- the snyk vulnerability identifier

For example,
SNYK-ALPINE321-EXPAT-15199474

When the Trail is attested to it first does a kosli-attest-artifact so non-compliance will ripple 
into any Env that has trail-compliance in an attached Policy.

Inputs from each snyk-scan for a custom-attestation
1. The timestamp of the trail is the start-date.
   curl -X 'GET' \
  'https://app.kosli.com/api/v2/trails/cyber-dojo/differ-ci/ac3a41d1c764bbdf71dcaf15018ad0a008f0dc1b' \
  -H 'accept: application/json'
  Gives...
  { "created_at": 1770627481.667767, ...}

2. The sarif file. Map to a dict with keys for each snyk vuln,
   with values {"level":"low", ...? }

3. The .snyk file? This would a very nice extra.
   Could you add entries into the map from 2) for each entry in the .snyk file?
   Just because a vuln is named in a .snyk file does not mean the Artifact has that vuln.
   Idea: Specify that the .snyk file must never have any ignores! Let the control handle it.
   This does not work for long running vulns that do not get fixed quickly. Eg runner's.
   That suggests it is better to simply assume the .snyk file tells the truth.
   However, the .snyk file does not specify the vuln level, only its ID.
   Is there a way to find the level with a snyk API call?
   That would avoid assuming it is in the .shortDescription.text (see below)
   The actual score is present. See below.

4. The current timestamp.
   Is there a jq expression for this?
   Apparently yes: now

5. The max age (in days?) for each level.
   No. This should be set in the custom-attestation's type.

Should there also be a maximum number of vulns at a specific level?
eg low=100, medium=20, high=5, critical=1
Maybe - but that is a different policy.

If an Artifact has a given snyk vuln, and a new artifact is deployed to replace it 
before it "expires" and becomes non-compliant, then it the Trail never becomes non-compliant 
because the Artifact is not seen in later live-snyk-scans. So there is never a later 
attestation which trips it into red.

Suppose an Artifact is deployed to aws-beta and then to aws-prod. 
Why should the start-date (for the max-age) come from the Trail connected to aws-prod?
It was seen in aws-beta first. And in reality the Env is irrelevant, since the fingerprint 
will match regardless of the Flow or Trail.

Sometimes emergency rollback deployments are necessary. Suppose an Artifact is 
rolled back. It will then be seen in a live-snyk-scan, and a new attestation will be made.
This may well cause the Trail to become non-compliant. I think this is correct.
Note that there is no way that a given Artifact can ever "not have" the given vulnerability. 
You can only fix a non-compliance by deploying a new Artifact that has fixed the given vuln.

Should all Trails go under a single Flow?
Can a case be made for having a Flow per quarter - for example if you need 
a quarterly report on the compliance of this specific control?
I think it is better to have a single Flow, and use a date range filter for the report.

It is interesting that the max-age determination is only applied when an attestation 
is made. It would be better if it was re-determined at the point you asked. For example,
Artifact F1 might be compliant in Kosli, but if a live-snyk-scan was done NOW it would 
become non-compliant. Nevertheless, if you ask if F1 is compliant, Kosli will say yes 
because it can only see the date of the most recent attestation.


Sarif file processing
---------------------

runs[@].tool.driver.rules[@]
  .id                           eg "SNYK-ALPINE321-EXPAT-15199474"
  .shortDescription.text        eg "Low severity - Integer Overflow..."
  .cvssv3_baseScore             eg 2.5, Note: sometimes == null
  .properties.security-severity eg "2.5" Note: Sometimes == "null"!

Another example:

runs[@].tool.driver.rules[@]
  .id                           eg "SNYK-GOLANG-GITHUBCOMOPENCONTAINERSSELINUXGOSELINUX-13843568"
  .shortDescription.text        eg "High severity - Race Condition Enabli..."
  .cvssv3_baseScore             eg 7.3,
  .properties.security-severity eg "7.3" 

From https://support.snyk.io/s/article/How-is-a-vulnerabilitys-severity-determined

At Snyk, we use CVSS framework version 3.1 to communicate the 
characteristics and severity of vulnerabilities.
A vulnerability's severity (critical, high, medium or low) is based on its CVSS score:

  Severity	CVSS v3 Rating
  ------------------------
  Critical	9.0 - 10.0
  High	    7.0 -  8.9
  Medium	  4.0 -  6.9
  Low	      0.1 -  3.9


The Flow the attestations are written to will not be aws-snyk-scan since that does 
not implement the policy of max-age per vuln severity. 
Create a new Flow called snyk-vulnerabilities.
Its Trails will be named based on fingerprint@vuln, eg
21aeb61ece58bab3221fde0ec9ad26d8b11a0c12b04ad4162e0f8dfb2b5f47c9@SNYK-ALPINE321-EXPAT-15199474

The Env will rely on a Trail-compliance env-policy.
Note that a weakness of this approach is that an Env will no longer become 
non-compliant if a snyk-attestation is MISSING. Is there a way this can be overcome?
Yes. See below. The clock needs to start when deployment has finished.
The snyk-scan and attestation can be made in the main workflow after the deployment!

Attestations to this Trail will be custom attestations.
This can have the same name as the Flow - snyk-vulnerabilities
They will implement max-age policy based on severity.

  Critical	 7 days
  High	    14 days
  Medium	  21 days
  Low	      28 days

The jq rule will accept three arguments:
- timestamp for when Trail was created (could this be held as a tag on the Trail?)
- timestamp now (can jq supply this itself?)
- severity (critical, high, medium, low)

The main.yml workflows can attest to this Flow/Trail.
The workflows in the live-snyk-scans will also attest to the _same_ Flow/Trail.

The old Flow called aws-snyk-scan can be deleted.

Some severities are initially "null".
How to handle these?
Suppose they default to medium?
That will work.
If, in a later snyk-scan it becomes "high" then the attestation 
for that scan will pass in "high" as input to the custom-attestation.

Note that a customer might want to find all Artifacts with a given
vulnerability. Is this possible?

Does it make sense for the custom-attestation max-age thresholds to
be different for aws-beta vs aws-prod?
Eg critical can be 10 days in aws-beta but only 1 day in prod?
I think yes. 
If so does it make sense (work?) for there to be 2 Flows?
And how does this work with the notion that what makes the Env 
turn green is Trail-compliance?
I don't think I can easily do it based on Trail compliance since 
the fingerprint will match regardless of the Env.
It has to be a specific custom-attestation.

1. You have to have a Flow + Trails dedicated to the Env.
2. You have to make one of these custom attestations in the build process
   "close" to deployment. The clock starts not when the Artifact is built,
   but when it is deployed. Probably best is to run the attest job in parallel
   with the deploy job. If it is after then you will get the odd "false" red 
   snapshot for the Env because you'll get a snapshot report just before the 
   attestation.
3. You can also make these custom-attestations in the live-snyk-scan process.
4. You have to have a Policy that requires the custom-attestation like this:

  attestations:
    - if: ${{ flow.name == 'snyk-vuln-ages-on-aws-beta' }}
      name: snyk-vuln-ages-on-aws-beta
      type: custom:snyk-vuln-ages-on-aws-beta

This is not bad. It means a rogue deployment won't have the attestation.

Would be nice if the severity of the vuln was easily visible.
You cannot tag a Trail, only a Flow (or Env).
Simplest is to add it to the Trail name. Eg

low.SNYK-ALPINE321-EXPAT-15199474

Note that "Configure AWS credentials" step relies on OIDC permissions 
which will need to be configured for both the build workflow in the Artifact's repo 
and the live-snyk-scan workflow in this repo.

Maybe have 4 Flows per Env? One for each vuln level?

If you have a Trail per vuln, then you will get several Artifacts with that vuln.
This is ok, since each Artifact can attest its own fingerprint and thus 
be found when a new snapshot is being evaluated.
However, it does mean that using the start-date of the Trail will start 
the clock for all Artifacts. 
Is it better for the Trail name to be a combination of the 
fingerprint and the vuln?
Maybe both?
Consider if the vuln is critical
1. From the env point of view, what matters is when it first appears - regardless 
of how many running Artifacts exhibit it.
2. From the Trail's point of view, what matters is when that artifact was deployed
if the policy is that it must be fixed within a certain period of time.

Suppose 1 < 2 
Eg Env non-compliant for critical vuln running for >= 1 day.
   Trail non-compliant for critical vuln known for >= 10 days.
In this case you can deploy through the sdlc gate, but will become
non-compliant once deployed.

Suppose 1 > 2
Eg Env non-compliant for critical vuln running for >= 10 day.
   Trail non-compliant for any critical vuln - known for >= 0 days.
In this case you can't deploy. 

What is the behaviour we want?

Suppose I am a dev and there is a policy that ANY high vuln must be fixed 
within 5 days of first appearance in an Env.
This affects my sdlc gate for deploying that artifact to that Env.
The assert-artifact (--environment) evaluation needs to somehow get the timestamp 
of first Artifact with that vuln running in that Env (which defaults to now if none yet).

This is doable if creation timestamps is passed explicitly to the custom-attestation

Example...

Flow=snyk-vuln-ages-on-aws-beta
Trail=low.SNYK-ALPINE321-EXPAT-15199474

Creation of this Trail is attempted when this vuln is seen in aws-beta
Then attest the fingerprint.
Then custom-attestion whose type is based on Flow.
  snyk-vuln-ages-on-aws-beta
  - 1st timestamp == creation timestamp of Trail 
  - 2nd timestamp == now... is it needed? 
    This is really the timestamp of the attestation itself.
    Is 'now' in jq expression of the Env-Policy enough?
  - level.  critical,high,medium,low

What happens when a new snapshot for aws-beta comes in?
Fingerprint finds Trail
Env Policy is attached to aws-beta so is evaluated.

Now, if the Trail has 3 Artifacts, then 3rd one was 3rd,
but the timestamp of all attestations is the same, the timestamp
of the Trail. So all Artifacts become non-compliant because they 
are in the same Trail, via Trail-compliance.

Now suppose there are two low vulns...

Trail=low.SNYK-ALPINE321-EXPAT-15199474
Trail=low.SNYK-ALPINE321-EXPAT-15199475

One is seen Monday, 2nd is seen Tuesday.
So there will be two clocks.
But that is correct. 
This is not a policy on how many low vulns there are.
It is a policy on how long you've got to fix each SPECIFIC low vuln.

Now suppose an Artifact with low.SNYK-ALPINE321-EXPAT-15199474 is deployed to aws-beta.
It lives for a while and is replaced by a blue-green deployment.
Now, several weeks later, there is a "regression" and some Artifact with 
this vuln appears at the sdlc-gate. It could easily be immediately be non-compliant.
But maybe this is what you want?
This is not about preventing deployment. 
It is about how long an Artifact is allowed to have the vuln.

Regardless, I'd like to experiment with an Env-Policy whose jq rule uses 'now'



A cron-job scrapes a aws-beta snapshot json and does a snyk-scan for each Artifact.
Each snyk-scan generates a sarif file.
Small python script extracts each vuln name and its level (eg medium).
In a flow called aws-beta-snyk-vuln-ages, we create a Trail.
Name of the Trail is combination of Artifact fingerprint and vuln name.
Now get the JSON for the trail and extract "creation" timestamp.
kosli-attest-artifact to stamp fingerprint into Trail.
kosli-attest-custom passing in creation timestamp and vuln level.

Now have an Env-Policy 

Core ideas.

Have a Flow dedicated to the control, eg aws-beta-snyk-vuln-ages
Create Trails on this Flow with identity based on fingerprint and vuln name.
Have custom attestation again based on the control which attests only to these Trails.
Input data for the custom-attestation is 
  - "creation" timestamp of Trail
  - vuln level
Custom attestation embeds rule, eg
 vuln=="medium" => 'now' - creation <= 7 days.
(now is a jq expression)

Assumption: each new snapshot report (eg every 3 mins) does a re-evaluation of
compliance and this re-evaluates 'now' in each found custom-attestation.

These Trails will be found when asking if the Artifact "would be" compliant if deployed 
to different Env. So Env-Policies have to have a way being "specific enough".
This could be via filter on flow name or flow tag (can't tag trails)
Or env name or env tag.
Or it could be via type-name of the custom-attestation.

A customer might want to "allow-list" (or un-allow-list) not at the Artifact level, but at 
the "control" level for that Artifact. Which in this case would equate to the specific Trail's 
overall trail-compliance.
Eg I might want to allow-list compliance of Trail
  12345678.SNYK-ALPINE321-EXPAT-15199474
but not of 
  87654321.SNYK-ALPINE321-EXPAT-15199474


Idea:
When you get the snapshot of aws-beta, the Artifacts found should be snyk scanned 
and then an attestation made to TWO flows, one for the aws-beta env it is currently 
in, and one for the aws-prod env it may get promoted to.

Flow: aws-beta-snyk-vuln-ages
Trail: digest.low.SNYK-ALPINE321-EXPAT-15199474
CAT: aws-beta-snyk-vuln-age
  vuln=="medium" => 'now' - creation <= 7 days.

Flow: aws-prod-snyk-vuln-ages
Trail: digest.low.SNYK-ALPINE321-EXPAT-15199474
CAT: aws-prod-snyk-vuln-age
  vuln=="medium" => 'now' - creation <= 1 days.

A critical vuln could be prevented from ever reaching prod 
by having a vuln=="critical" rule with no part for its age.

Now have TWO Env policies

Attached to aws-beta only:
CAT aws-beta-snyk-vuln-age

Attached to aws-prod only:
CAT aws-prod-snyk-vuln-age

These attestations must ALSO be made just before deployment.

However, this might not work since if the attestation to aws-prod-snyk-vuln-ages
is red, then if aws-beta has a global Trail-compliance Env-policy then that will fail.
So Trail-compliance policy has to be tied to specific Flow or Tag.

There is still a problem. If there are no vulns for an Artifact then
there will be no attestations, and the Env-Policy will fail!
So in that case we have to somehow ensure there is a TRUE cat.
How to do that? Some kind of "override" argument?
Perhaps in a Trail called digest.low.NONE

-----------------------------------

Key point: the cron job that runs every day, finds live artifacts 
and does a snyk-container-tests WITH the .snyk file
It then makes attestations for what will be NEW vulns, with an aged based flow/trail.

