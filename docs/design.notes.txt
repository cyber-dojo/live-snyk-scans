
It would be useful if each repo could still have its own snyk scan if desired.
Could be a separate workflow? One issue is that if the main workflow runs 
then it will create a docker image in the registry with an immutable tag,
so the snyk workflow would not be able to build its image in the same way.

Could the main workflow have a snyk-container-scan job that does not participate 
in the sdlc-control-gate and so does not affect deployment?
This would mean the scan is definitely on the correct docker image.

Can a snyk scan work on a docker image saved to a tar file?

I could have Trails in a Flow=aws-snyk-scan whose names are taken from
- the fingerprint
- the snyk vulnerability identifier

For example,
21aeb61ece58bab3221fde0ec9ad26d8b11a0c12b04ad4162e0f8dfb2b5f47c9@SNYK-ALPINE321-EXPAT-15199474
or
SNYK-ALPINE321-EXPAT-15199474@21aeb61ece58bab3221fde0ec9ad26d8b11a0c12b04ad4162e0f8dfb2b5f47c9

When the Trail is created it does a kosli-attest-artifact so non-compliance will ripple 
into any Env that has trail-compliance in an attached Policy.

Inputs for each snyk-scan:
1. The timestamp of the trail is the start-date.
2. The sarif file. Map to a dict with keys for each snyk vuln,
   with values {"level":"low", ...? }
3. The .snyk file? This would a very nice extra.
   Could you add entries into the map from 2) for each entry in the .snyk file?
   Just because a vuln is named in a .snyk file does not mean the Artifact has that vuln.
   Idea: Specify that the .snyk file must never have any ignores! Let the control handle it.
   This does not work for long running vulns that do not get fixed quickly. Eg runner's.
   That suggests it is better to simply assume the .snyk file tells the truth.
   However, the .snyk file does not specify the vuln level, only its ID.
   Is there a way to find the level with a snyk API call?
   That would avoid assuming it is in the .shortDescription.text (see below)
   The actual score is present. See below.
4. The current timestamp
5. The max age (in days?) for each level. Defaults?

Should there also be a maximum number of vulns at a specific level?
eg low=100, medium=20, high=5, critical=1

If an Artifact has a given snyk vuln, and a new artifact is deployed to replace it 
before it "expires" and becomes non-compliant, then it the Trail never becomes non-compliant 
because the Artifact is not seen in later live-snyk-scans. So there is never a later 
attestation which trips it into red.

Suppose an Artifact is deployed to aws-beta and then to aws-prod. 
Why should the start-date (for the max-age) come from the Trail connected to aws-prod?
It was seen in aws-beta first. And in reality the Env is irrelevant, since the fingerprint 
will match regardless of the Flow or Trail.

Sometimes emergency rollback deployments are necessary. Suppose an Artifact is 
rolled back. It will then be seen in a live-snyk-scan, and a new attestation will be made.
This may well cause the Trail to become non-compliant. I think this is correct.
Note that there is no way that a given Artifact can ever "not have" the given vulnerability. 
You can only fix a non-compliance by deploying a new Artifact that has fixed the given vuln.

Should all Trails go under a single Flow?
Can a case be made for having a Flow per quarter - for example if you need 
a quarterly report on the compliance of this specific control?
I think it is better to have a single Flow, and use a date range filter for the report.

It is interesting that the max-age determination is only applied when an attestation 
is made. It would be better if it was re-determined at the point you asked. For example,
Artifact F1 might be compliant in Kosli, but if a live-snyk-scan was done NOW it would 
become non-compliant. Nevertheless, if you ask if F1 is compliant, Kosli will say yes 
because it can only see the date of the most recent attestation.


Sarif file processing
---------------------

runs[@].tool.driver.rules[@]
  .id                           eg "SNYK-ALPINE321-EXPAT-15199474"
  .shortDescription.text        eg "Low severity - Integer Overflow..."
  .cvssv3_baseScore             eg 2.5, Note: sometimes == null
  .properties.security-severity eg "2.5" Note: Sometimes == "null"!

Another example:

runs[@].tool.driver.rules[@]
  .id                           eg "SNYK-GOLANG-GITHUBCOMOPENCONTAINERSSELINUXGOSELINUX-13843568"
  .shortDescription.text        eg "High severity - Race Condition Enabli..."
  .cvssv3_baseScore             eg 7.3,
  .properties.security-severity eg "7.3" 

From https://support.snyk.io/s/article/How-is-a-vulnerabilitys-severity-determined

At Snyk, we use CVSS framework version 3.1 to communicate the 
characteristics and severity of vulnerabilities.
A vulnerability's severity (critical, high, medium or low) is based on its CVSS score:

  Severity	CVSS v3 Rating
  ------------------------
  Critical	9.0 - 10.0
  High	    7.0 -  8.9
  Medium	  4.0 -  6.9
  Low	      0.1 -  3.9


The Flow the attestations are written to will notion be aws-snyk-scan since that does 
not implement the policy of max-age per vuln severity. 
Create a new Flow called snyk-vulnerabilities.
Its Trails will be named based on fingerprint@vuln, eg
21aeb61ece58bab3221fde0ec9ad26d8b11a0c12b04ad4162e0f8dfb2b5f47c9@SNYK-ALPINE321-EXPAT-15199474

When a Trail is created it will also need a _single_ kosli-attest-artifact
(so the Trail will be found by Env Policies). 
The Env will rely on a Trail-compliance env-policy.
Note that a weakness of this approach is that an Env will no longer become 
non-compliant if a snyk-attestation is MISSING. Is there a way this can be overcome?
Yes. See below. The clock needs to start when deployment has finished.
The snyk-scan and attestation can be made in the main workflow after the deployment!

Attestations to this Trail will be custom attestations.
This can have the same name as the Flow - snyk-vulnerabilities
They will implement max-age policy based on severity.

  Critical	 7 days
  High	    14 days
  Medium	  21 days
  Low	      28 days

The jq rule will accept three arguments:
- timestamp for when Trail was created (could this be held as a tag on the Trail?)
- timestamp now (can jq supply this itself?)
- severity (critical, high, medium, low)

The main.yml workflows can attest to this Flow/Trail.
The workflows in the live-snyk-scans will also attest to the _same_ Flow/Trail.

The old Flow called aws-snyk-scan can be deleted.

Some severities are initially "null".
How to handle these?
Suppose they default to medium?
That will work.
If, in a later snyk-scan it becomes "high" then the attestation 
for that scan will pass in "high" as input to the custom-attestation.

Note that a customer might want to find all Artifacts with a given
vulnerability. Is this possible?

Does it make sense for the custom-attestation max-age thresholds to
be different for aws-beta vs aws-prod?
Eg critical can be 10 days in aws-beta but only 1 day in prod?
I think yes. 
If so does it make sense (work?) for there to be 2 Flows?
And how does this work with the notion that what makes the Env 
turn green is Trail-compliance?
I don't think I can easily do it based on Trail compliance since 
the fingerprint will match regardless of the Env.
It has to be a specific custom-attestation.

1. You have to have a Flow + Trails dedicated to the Env.
2. You have to make one of these custom attestations in the build process
   "close" to deployment. The clock starts not when the Artifact is built,
   but when it is deployed. Probably best is to run the attest job in parallel
   with the deploy job. If it is after then you will get the odd "false" red 
   snapshot for the Env because you'll get a snapshot report just before the 
   attestation.
3. You can also make these custom-attestations in the live-snyk-scan process.
4. You have to have a Policy that requires the custom-attestation like this:

  attestations:
    - if: ${{ flow.name == 'snyk-vuln-ages-on-aws-beta' }}
      name: snyk-vuln-ages-on-aws-beta
      type: custom:snyk-vuln-ages-on-aws-beta

This is not bad. It means a rogue deployment won't have the attestation.

Would be nice if the severity of the vuln was easily visible.
You cannot tag a Trail, only a Flow (or Env).
Simplest is to add it to the Trail name. Eg

21aeb61ece58bab3221fde0ec9ad26d8b11a0c12b04ad4162e0f8dfb2b5f47c9@low.SNYK-ALPINE321-EXPAT-15199474
Better is
21aeb61ece58bab3221fde0ec9ad26d8b11a0c12b04ad4162e0f8dfb2b5f47c9.low.SNYK-ALPINE321-EXPAT-15199474

Note that "Configure AWS credentials" step relies on OIDC permissions 
which will need to be configured for both the build workflow in the Artifact's repo 
and the live-snyk-scan workflow in this repo.
